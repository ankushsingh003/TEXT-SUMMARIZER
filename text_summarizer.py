# -*- coding: utf-8 -*-
"""Text_summarizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NGIISif_DlaK5C78IXumq7kHhgL_M5I_
"""

!nvidia-smi

!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q

!pip install --upgrade accelerate
!pip uninstall -y transformers accelerate
!pip install transformers accelerate

from transformers import pipeline, set_seed
from datasets import  load_dataset , load_from_disk
import matplotlib.pyplot as plt
import pandas as pd
# from datasets import load_dataset , load_metric
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import nltk
from nltk.tokenize import sent_tokenize
nltk.download("punkt")
from tqdm import tqdm
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

model = "google/pegasus-cnn_dailymail"
tokenizer = AutoTokenizer.from_pretrained(model)

model_mail = AutoModelForSeq2SeqLM.from_pretrained(model).to(device)

!pip install huggingface_hub

from huggingface_hub import login
login()

from huggingface_hub import login
from google.colab import userdata

try:
    hf_token = userdata.get('HF_TOKEN')
    login(token=hf_token)
    print("Successfully logged in to Hugging Face Hub.")
except Exception as e:
    print(f"Could not log in to Hugging Face Hub: {e}")
    print("Please ensure your HF_TOKEN is correctly set in Colab secrets.")

dataset_mail = load_dataset("cnn_dailymail", '3.0.0')
dataset_mail

dataset_mail

dataset_mail["train"]["highlights"][1]

dataset_mail["train"]["article"][1]

dataset_mail["train"][1]["highlights"]

split_lengths = [len(dataset_mail[split])for split in dataset_mail.keys()]
print(f"Split lengths: {split_lengths}")

print(f"Features: {dataset_mail['train'].column_names}")
print(dataset_mail["test"][1]["highlights"])
print("\nSummary")
print(dataset_mail["test"][1]["article"])

"""NOW WE NEED TO CONVERT THIS WORDS INTO VECTOR
SO WE NEED TO CONVERT INTO VECTORS
WE WILL USE TOKENIZER
"""

def convert_examples_to_features(example_batch):
  input = tokenizer(example_batch['article'],max_length=100,truncation=True)
  with tokenizer.as_target_tokenizer():
    target = tokenizer(example_batch['highlights'],max_length=64,truncation=True)

  return{
      'input_ids':input['input_ids'],
      'attention_mask':input['attention_mask'],
      'labels':target['input_ids']
  }

dataset_mapped = dataset_mail.map(convert_examples_to_features,batched=True)

dataset_mapped["train"]

dataset_mapped["train"]["input_ids"][1]

dataset_mapped["train"]["labels"][1]

dataset_mapped["train"]["attention_mask"][1]

"""TRAINING"""

from transformers import DataCollatorForSeq2Seq
seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer,model=model_mail)

from transformers import TrainingArguments, Trainer
trainer_args = TrainingArguments(
    output_dir='cnn-mail',num_train_epochs=1,warmup_steps=500,
    per_device_train_batch_size=1 , per_device_eval_batch_size = 1 ,
    weight_decay = 0.01 , logging_steps = 10 ,
    # evaluation_strategy = 'steps', eval_steps = 500 ,  # Temporarily removed to resolve TypeError
    save_steps = 1e6,
    gradient_accumulation_steps = 16
)

trainer = Trainer( model = model_mail , args = trainer_args ,
                  tokenizer = tokenizer , data_collator = seq2seq_data_collator ,
                  train_dataset = dataset_mapped["train"],
                  eval_dataset = dataset_mapped["validation"])

trainer.train()

"""EVALUATION"""

# Evaluation

def generate_batch_sized_chunks(list_of_elements, batch_size):
    """
    Split the dataset into smaller batches that we can process simultaneously.
    Yield successive batch-sized chunks from list_of_elements.
    """
    for i in range(0, len(list_of_elements), batch_size):
        yield list_of_elements[i : i + batch_size]


def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,
                                batch_size=16, device=device,
                                column_text="article",
                                column_summary="highlights"):

    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))
    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))

    for article_batch, target_batch in tqdm(
        zip(article_batches, target_batches), total=len(article_batches)):
        inputs = tokenizer(article_batch, max_length=1024, truncation=True,
                   padding="max_length", return_tensors="pt")

        summaries = model.generate(
        input_ids=inputs["input_ids"].to(device),
        attention_mask=inputs["attention_mask"].to(device),
        length_penalty=0.8, num_beams=8, max_length=128)


        decoded_summaries = [tokenizer.decode(
                        s, skip_special_tokens=True,
                        clean_up_tokenization_spaces=True)
                     for s in summaries]

        decoded_summaries = [d.replace("", " ") for d in decoded_summaries]

        metric.add_batch(predictions=decoded_summaries, references=target_batch)

    score = metric.compute()
    return score

rouge_names = ["rouge1" , "rouge2" , "rougeL" , "rougeLsum"]
rouge_metric = load_metric('rouge')

score = calculate_metric_on_test_ds(
    dataset_mail['test'],
    rouge_metric,
    trainer.model,
    tokenizer,
    batch_size = 2,
    column_text = 'article',
    column_summary= 'highlights'
)
rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )
pd.DataFrame(rouge_dict, index = [f'mail'])

"""SAVE MODEL"""

model_mail.save.pretrained("cnn_dailymail")

tokenizer.save_pretrained("tokenizer")

"""PREDICT"""

gen_kwargs = {"length_penalty": 0.8, "num_beams":8, "max_length": 128}
sampple_text = dataset_mail["test"][0]["article"]
reference = dataset_mail["test"][0]["highlights"]
pipe = pipeline("summarization", model="cnn_dailymail", tokenizer="tokenizer")
print("Dialogue:")
print(sample_text)
print("\nReference Summary:")
print(reference)
print("\nModel Summary:")

